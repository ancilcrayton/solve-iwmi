{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597350883225",
   "display_name": "Python 3.8.5 64-bit ('solve-iwmi': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling using LDA (Latent  Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import eland as ed\n",
    "from eland.conftest import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "from pprint import pprint\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel, Phrases\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data from Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_df = ed.DataFrame('localhost', 'twitter', columns=['full_text_processed', 'user_id', 'verified', 'name', 'location', 'entities.hashtags.text', 'entities.user_mentions.name'])\n",
    "\n",
    "# defining the full-text query we need: Retrieving records for full_text_processed with the condition is_retweet=False and is_quote_status=False\n",
    "query_unique = {\n",
    "    \"bool\": {\n",
    "        \"must\": {\n",
    "            \"term\":{\"is_retweet\":\"false\"},\n",
    "        },\n",
    "        \"filter\": {\n",
    "            \"term\":{\"is_quote_status\":\"false\"}\n",
    "        },\n",
    "    }\n",
    "}\n",
    "# using full-text search capabilities with Eland:\n",
    "df_ed = ed_df.es_query(query_unique)\n",
    "df_tweets = df_ed.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                   full_text_processed              user_id  \\\n1264160647002103808  praying everyone affected condolence family vi...  1256622599364214786   \n1264160609668599808  cyclone ampan people satkhira upset due lack w...  1251934220345208832   \n1264121161589415936  cyclone amphan ha completely destroyed agricul...  1251934220345208832   \n1264160569315209216  amphan cyclone ‚Äã‚Äãcm mamta demand ban labor spe...  1113075640499036160   \n1264114187346874368  amfan storm caused devastation bengal mp nusra...  1113075640499036160   \n\n                     verified         name               location entities.hashtags.text  \\\n1264160647002103808     False  The Meraaki  Ahmadabad City, India     AmphanSuperCyclone   \n1264160609668599808     False   Newspapers                  Dhaka                    NaN   \n1264121161589415936     False   Newspapers                  Dhaka                    NaN   \n1264160569315209216     False      netvani                   None                    NaN   \n1264114187346874368     False      netvani                   None                    NaN   \n\n                    entities.user_mentions.name  \n1264160647002103808                         NaN  \n1264160609668599808                         NaN  \n1264121161589415936                         NaN  \n1264160569315209216                         NaN  \n1264114187346874368                         NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>full_text_processed</th>\n      <th>user_id</th>\n      <th>verified</th>\n      <th>name</th>\n      <th>location</th>\n      <th>entities.hashtags.text</th>\n      <th>entities.user_mentions.name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1264160647002103808</th>\n      <td>praying everyone affected condolence family vi...</td>\n      <td>1256622599364214786</td>\n      <td>False</td>\n      <td>The Meraaki</td>\n      <td>Ahmadabad City, India</td>\n      <td>AmphanSuperCyclone</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1264160609668599808</th>\n      <td>cyclone ampan people satkhira upset due lack w...</td>\n      <td>1251934220345208832</td>\n      <td>False</td>\n      <td>Newspapers</td>\n      <td>Dhaka</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1264121161589415936</th>\n      <td>cyclone amphan ha completely destroyed agricul...</td>\n      <td>1251934220345208832</td>\n      <td>False</td>\n      <td>Newspapers</td>\n      <td>Dhaka</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1264160569315209216</th>\n      <td>amphan cyclone ‚Äã‚Äãcm mamta demand ban labor spe...</td>\n      <td>1113075640499036160</td>\n      <td>False</td>\n      <td>netvani</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1264114187346874368</th>\n      <td>amfan storm caused devastation bengal mp nusra...</td>\n      <td>1113075640499036160</td>\n      <td>False</td>\n      <td>netvani</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenising and removing short tweets (less than 4 words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['full_text_processed'] = df_tweets['full_text_processed'].apply(lambda x: remove_emoji(x))\n",
    "df_tweets['full_text_tokens'] = df_tweets['full_text_processed'].apply(lambda x: [w for w in x.split()])\n",
    "df_tweets['length'] = df_tweets['full_text_tokens'].apply(lambda x: len(x))\n",
    "df_tweets = df_tweets[df_tweets['length']>4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                   full_text_processed              user_id  \\\n1264160647002103808  praying everyone affected condolence family vi...  1256622599364214786   \n1264160609668599808  cyclone ampan people satkhira upset due lack w...  1251934220345208832   \n1264121161589415936  cyclone amphan ha completely destroyed agricul...  1251934220345208832   \n1264160569315209216  amphan cyclone ‚Äã‚Äãcm mamta demand ban labor spe...  1113075640499036160   \n1264114187346874368  amfan storm caused devastation bengal mp nusra...  1113075640499036160   \n\n                     verified         name               location entities.hashtags.text  \\\n1264160647002103808     False  The Meraaki  Ahmadabad City, India     AmphanSuperCyclone   \n1264160609668599808     False   Newspapers                  Dhaka                    NaN   \n1264121161589415936     False   Newspapers                  Dhaka                    NaN   \n1264160569315209216     False      netvani                   None                    NaN   \n1264114187346874368     False      netvani                   None                    NaN   \n\n                    entities.user_mentions.name  \\\n1264160647002103808                         NaN   \n1264160609668599808                         NaN   \n1264121161589415936                         NaN   \n1264160569315209216                         NaN   \n1264114187346874368                         NaN   \n\n                                                      full_text_tokens  length  \n1264160647002103808  [praying, everyone, affected, condolence, fami...       8  \n1264160609668599808  [cyclone, ampan, people, satkhira, upset, due,...       9  \n1264121161589415936  [cyclone, amphan, ha, completely, destroyed, a...       9  \n1264160569315209216  [amphan, cyclone, ‚Äã‚Äãcm, mamta, demand, ban, la...      15  \n1264114187346874368  [amfan, storm, caused, devastation, bengal, mp...       9  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>full_text_processed</th>\n      <th>user_id</th>\n      <th>verified</th>\n      <th>name</th>\n      <th>location</th>\n      <th>entities.hashtags.text</th>\n      <th>entities.user_mentions.name</th>\n      <th>full_text_tokens</th>\n      <th>length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1264160647002103808</th>\n      <td>praying everyone affected condolence family vi...</td>\n      <td>1256622599364214786</td>\n      <td>False</td>\n      <td>The Meraaki</td>\n      <td>Ahmadabad City, India</td>\n      <td>AmphanSuperCyclone</td>\n      <td>NaN</td>\n      <td>[praying, everyone, affected, condolence, fami...</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>1264160609668599808</th>\n      <td>cyclone ampan people satkhira upset due lack w...</td>\n      <td>1251934220345208832</td>\n      <td>False</td>\n      <td>Newspapers</td>\n      <td>Dhaka</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[cyclone, ampan, people, satkhira, upset, due,...</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1264121161589415936</th>\n      <td>cyclone amphan ha completely destroyed agricul...</td>\n      <td>1251934220345208832</td>\n      <td>False</td>\n      <td>Newspapers</td>\n      <td>Dhaka</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[cyclone, amphan, ha, completely, destroyed, a...</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1264160569315209216</th>\n      <td>amphan cyclone ‚Äã‚Äãcm mamta demand ban labor spe...</td>\n      <td>1113075640499036160</td>\n      <td>False</td>\n      <td>netvani</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[amphan, cyclone, ‚Äã‚Äãcm, mamta, demand, ban, la...</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>1264114187346874368</th>\n      <td>amfan storm caused devastation bengal mp nusra...</td>\n      <td>1113075640499036160</td>\n      <td>False</td>\n      <td>netvani</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[amfan, storm, caused, devastation, bengal, mp...</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Bigram and Trigram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from','not', 'would', 'say', 'could', '_', 'be', 'go', 'do', 'rather', 'seem', 'due', 'via', 'done', 'said'])\n",
    "\n",
    "tweets_list = df_tweets.full_text_tokens.to_list()\n",
    "tweet_ids = df_tweets.index.to_list()\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "\n",
    "bigram = Phrases(tweets_list, min_count=10, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = Phrases(bigram[tweets_list], threshold=100)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [[word for word in gensim.utils.simple_preprocess(str(tweet))] for tweet in tweets_list]\n",
    "tweets = [bigram_mod[tweet] for tweet in tweets]\n",
    "tweets = [trigram_mod[bigram_mod[tweet]] for tweet in tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "tweets_dict = corpora.Dictionary(tweets)\n",
    "\n",
    "# Filtering extremes by removing tokens occuring in less than 10 tweets and have occured in more than 90% tweets\n",
    "tweets_dict.filter_extremes(no_below=10, no_above=0.9)\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [tweets_dict.doc2bow(twt) for twt in tweets]\n",
    "\n",
    "# Adding the TF-IDF for better insight \n",
    "tfidf = gensim.models.TfidfModel(corpus)\n",
    "tfidf_corpus = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on Hyperparameter optimization - trying 2 approaches:\n",
    "- Topics = 6, Alpha = 0.01\n",
    "- Topics = 10, Alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA Model Parameters\n",
    "\n",
    "NUM_TOPICS_1 = 10\n",
    "ALPHA_1 = 1\n",
    "NUM_TOPICS_2 = 6\n",
    "ALPHA_2 = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_model_build(corpus, dictionary, topics, alpha, texts):\n",
    "    # Build LDA model\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=dictionary,\n",
    "                                            num_topics=topics, \n",
    "                                            random_state=100,\n",
    "                                            passes=10,\n",
    "                                            alpha=alpha,\n",
    "                                            per_word_topics=True)\n",
    "    \n",
    "    print(\"\\nModel, Topics=\",topics)\n",
    "    pprint(lda_model.print_topics())\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    return lda_model, coherence_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nModel, Topics= 10\n[(0,\n  '0.038*\"kolkata\" + 0.023*\"hurricane\" + 0.022*\"day\" + 0.021*\"disaster\" + '\n  '0.019*\"indian\" + 0.018*\"devastated\" + 0.017*\"many\" + 0.015*\"see\" + '\n  '0.013*\"part\" + 0.013*\"loss\"'),\n (1,\n  '0.040*\"ha\" + 0.032*\"amp\" + 0.031*\"help\" + 0.025*\"people\" + 0.020*\"bjp\" + '\n  '0.018*\"please\" + 0.015*\"home\" + 0.015*\"family\" + 0.013*\"victim\" + '\n  '0.012*\"get\"'),\n (2,\n  '0.038*\"time\" + 0.033*\"covid\" + 0.016*\"house\" + 0.016*\"due\" + 0.015*\"crisis\" '\n  '+ 0.013*\"coming\" + 0.012*\"lockdown\" + 0.011*\"devastating\" + 0.011*\"made\" + '\n  '0.010*\"already\"'),\n (3,\n  '0.036*\"via\" + 0.032*\"damage\" + 0.030*\"relief\" + 0.027*\"government\" + '\n  '0.026*\"amphan\" + 0.026*\"cyclone\" + 0.025*\"caused\" + 0.023*\"devastation\" + '\n  '0.022*\"live\" + 0.021*\"update\"'),\n (4,\n  '0.083*\"india\" + 0.063*\"bangladesh\" + 0.048*\"cyclone\" + 0.033*\"amphan\" + '\n  '0.025*\"dead\" + 0.025*\"people\" + 0.024*\"million\" + 0.023*\"make\" + '\n  '0.023*\"landfall\" + 0.020*\"least\"'),\n (5,\n  '0.021*\"corona\" + 0.017*\"like\" + 0.013*\"come\" + 0.013*\"nisarg\" + '\n  '0.012*\"earthquake\" + 0.012*\"nature\" + 0.010*\"govt\" + 0.010*\"go\" + '\n  '0.010*\"god\" + 0.009*\"didi\"'),\n (6,\n  '0.032*\"wa\" + 0.019*\"mumbai\" + 0.015*\"national\" + 0.014*\"still\" + '\n  '0.012*\"much\" + 0.011*\"service\" + 0.011*\"since\" + 0.011*\"medium\" + '\n  '0.011*\"know\" + 0.011*\"week\"'),\n (7,\n  '0.069*\"bengal\" + 0.056*\"west\" + 0.049*\"amfan\" + 0.048*\"odisha\" + '\n  '0.042*\"affected\" + 0.037*\"pm\" + 0.036*\"modi\" + 0.032*\"area\" + 0.025*\"crore\" '\n  '+ 0.019*\"due\"'),\n (8,\n  '0.031*\"nisarga\" + 0.020*\"year\" + 0.016*\"country\" + 0.015*\"pandemic\" + '\n  '0.015*\"two\" + 0.014*\"first\" + 0.013*\"new\" + 0.012*\"also\" + 0.012*\"kill\" + '\n  '0.011*\"another\"'),\n (9,\n  '0.052*\"storm\" + 0.036*\"hit\" + 0.033*\"news\" + 0.033*\"super\" + 0.027*\"may\" + '\n  '0.024*\"coast\" + 0.022*\"today\" + 0.021*\"wind\" + 0.020*\"ampan\" + '\n  '0.020*\"killed\"')]\n"
    }
   ],
   "source": [
    "# Build first model - Topics=10\n",
    "lda_model_1, score_1 = lda_model_build(corpus=tfidf_corpus, dictionary=tweets_dict, topics=NUM_TOPICS_1, alpha=ALPHA_1, texts=tweets)\n",
    "\n",
    "# Build first model - Topics=6\n",
    "lda_model_2, score_2 = lda_model_build(corpus=tfidf_corpus, dictionary=tweets_dict, topics=NUM_TOPICS_2, alpha=ALPHA_2, texts=tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model 1 - Topics = 10, Score = 0.4445514737374384\n"
    }
   ],
   "source": [
    "## Coherence Scores\n",
    "\n",
    "print(\"Model 1 - Topics = 10, Score =\",score_1)\n",
    "print(\"Model 2 - Topics = 6, Score =\",score_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further analysis on Model 1 (Topics = 10)\n",
    "Addressing certain questions and extracting more information out of the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Dominant Topic for each tweet and its percentage contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dominant_topic(lda_model, corpus):\n",
    "    tweet_topics = []\n",
    "    tweet_topics_percent = []\n",
    "    for tweet in tfidf_corpus:\n",
    "        topics_dist = lda_model.get_document_topics(tweet)\n",
    "        dom_topic, percent = max(topics_dist, key=lambda item:item[1])\n",
    "        tweet_topics.append(dom_topic)\n",
    "        tweet_topics_percent.append(percent)\n",
    "    return tweet_topics, tweet_topics_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_topics, tweet_topics_percent = get_dominant_topic(lda_model_1, tfidf_corpus) ## Storing the topic assignments for each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_topics_df = pd.DataFrame(list(zip(tweet_topics, tweet_topics_percent)), columns=['Topic', 'Percentage Contribution'], index=tweet_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                     Topic  Percentage Contribution\n1268051227734085632      3                 0.148246\n1268051184629223424      6                 0.140369\n1268051099086356480      0                 0.120297\n1268051025891430400      0                 0.150020\n1268050995482963968      1                 0.211865",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Topic</th>\n      <th>Percentage Contribution</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1268051227734085632</th>\n      <td>3</td>\n      <td>0.148246</td>\n    </tr>\n    <tr>\n      <th>1268051184629223424</th>\n      <td>6</td>\n      <td>0.140369</td>\n    </tr>\n    <tr>\n      <th>1268051099086356480</th>\n      <td>0</td>\n      <td>0.120297</td>\n    </tr>\n    <tr>\n      <th>1268051025891430400</th>\n      <td>0</td>\n      <td>0.150020</td>\n    </tr>\n    <tr>\n      <th>1268050995482963968</th>\n      <td>1</td>\n      <td>0.211865</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "tweet_topics_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_topics_df = pd.concat([df_tweets, tweet_topics_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                   full_text_processed              user_id  \\\n1264160647002103808  praying everyone affected condolence family vi...  1256622599364214786   \n1264160609668599808  cyclone ampan people satkhira upset due lack w...  1251934220345208832   \n1264121161589415936  cyclone amphan ha completely destroyed agricul...  1251934220345208832   \n1264160569315209216  amphan cyclone ‚Äã‚Äãcm mamta demand ban labor spe...  1113075640499036160   \n1264114187346874368  amfan storm caused devastation bengal mp nusra...  1113075640499036160   \n\n                     verified         name               location entities.hashtags.text  \\\n1264160647002103808     False  The Meraaki  Ahmadabad City, India     AmphanSuperCyclone   \n1264160609668599808     False   Newspapers                  Dhaka                    NaN   \n1264121161589415936     False   Newspapers                  Dhaka                    NaN   \n1264160569315209216     False      netvani                   None                    NaN   \n1264114187346874368     False      netvani                   None                    NaN   \n\n                    entities.user_mentions.name  \\\n1264160647002103808                         NaN   \n1264160609668599808                         NaN   \n1264121161589415936                         NaN   \n1264160569315209216                         NaN   \n1264114187346874368                         NaN   \n\n                                                      full_text_tokens  length  Topic  \\\n1264160647002103808  [praying, everyone, affected, condolence, fami...       8      1   \n1264160609668599808  [cyclone, ampan, people, satkhira, upset, due,...       9      0   \n1264121161589415936  [cyclone, amphan, ha, completely, destroyed, a...       9      6   \n1264160569315209216  [amphan, cyclone, ‚Äã‚Äãcm, mamta, demand, ban, la...      15      7   \n1264114187346874368  [amfan, storm, caused, devastation, bengal, mp...       9      6   \n\n                     Percentage Contribution  \n1264160647002103808                 0.158338  \n1264160609668599808                 0.178125  \n1264121161589415936                 0.186731  \n1264160569315209216                 0.182278  \n1264114187346874368                 0.163401  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>full_text_processed</th>\n      <th>user_id</th>\n      <th>verified</th>\n      <th>name</th>\n      <th>location</th>\n      <th>entities.hashtags.text</th>\n      <th>entities.user_mentions.name</th>\n      <th>full_text_tokens</th>\n      <th>length</th>\n      <th>Topic</th>\n      <th>Percentage Contribution</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1264160647002103808</th>\n      <td>praying everyone affected condolence family vi...</td>\n      <td>1256622599364214786</td>\n      <td>False</td>\n      <td>The Meraaki</td>\n      <td>Ahmadabad City, India</td>\n      <td>AmphanSuperCyclone</td>\n      <td>NaN</td>\n      <td>[praying, everyone, affected, condolence, fami...</td>\n      <td>8</td>\n      <td>1</td>\n      <td>0.158338</td>\n    </tr>\n    <tr>\n      <th>1264160609668599808</th>\n      <td>cyclone ampan people satkhira upset due lack w...</td>\n      <td>1251934220345208832</td>\n      <td>False</td>\n      <td>Newspapers</td>\n      <td>Dhaka</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[cyclone, ampan, people, satkhira, upset, due,...</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0.178125</td>\n    </tr>\n    <tr>\n      <th>1264121161589415936</th>\n      <td>cyclone amphan ha completely destroyed agricul...</td>\n      <td>1251934220345208832</td>\n      <td>False</td>\n      <td>Newspapers</td>\n      <td>Dhaka</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[cyclone, amphan, ha, completely, destroyed, a...</td>\n      <td>9</td>\n      <td>6</td>\n      <td>0.186731</td>\n    </tr>\n    <tr>\n      <th>1264160569315209216</th>\n      <td>amphan cyclone ‚Äã‚Äãcm mamta demand ban labor spe...</td>\n      <td>1113075640499036160</td>\n      <td>False</td>\n      <td>netvani</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[amphan, cyclone, ‚Äã‚Äãcm, mamta, demand, ban, la...</td>\n      <td>15</td>\n      <td>7</td>\n      <td>0.182278</td>\n    </tr>\n    <tr>\n      <th>1264114187346874368</th>\n      <td>amfan storm caused devastation bengal mp nusra...</td>\n      <td>1113075640499036160</td>\n      <td>False</td>\n      <td>netvani</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[amfan, storm, caused, devastation, bengal, mp...</td>\n      <td>9</td>\n      <td>6</td>\n      <td>0.163401</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "tweet_topics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the dataframe as a csv for future analysis\n",
    "\n",
    "tweet_topics_df.to_csv('../data/interim/tweet_topics_data_lda.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Tweets for Each Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       Count\nTopic       \n9      11110\n4      10793\n5      10188\n1      10181\n7       9696\n6       9664\n3       8984\n8       8537\n0       8508\n2       7676",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Count</th>\n    </tr>\n    <tr>\n      <th>Topic</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>9</th>\n      <td>11110</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10793</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>10188</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10181</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>9696</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>9664</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8984</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8537</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>8508</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7676</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 204
    }
   ],
   "source": [
    "top_topics = tweet_topics_df.groupby('Topic')\\\n",
    "    .size()\\\n",
    "    .to_frame()\\\n",
    "    .reset_index()\\\n",
    "    .rename(columns={0:'Count', 'Topic':'Topic'})\\\n",
    "    .set_index('Topic')\\\n",
    "    .nlargest(20, 'Count')\n",
    "\n",
    "top_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Representative tweet for each topic \n",
    "Tweet with highest contribution by the corresponding topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tweet_topics_df.groupby('Topic').apply(lambda x: x.sort_values('Percentage Contribution', ascending=False)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                     full_text_processed                    name\nTopic                                                                           \n0      despite early warning hurricane amfan mamata a...  Chowkidar Archya Midya\n1      first cyclone hard situation back front ha arr...            Moloy Ghatak\n2      thank sharing detail apology delay restricted ...                Tata Sky\n3      npr budget 3941 cr delhi power corridor 20000 ...       Citizen Bapan Das\n4      least 12 dead three million evacuee india bang...             S√≠ntesis TV\n5      786rizwankhan ja bhai apna kam kar kisne bola ...           Mohammed kaif\n6      climate crisis isnt coming cyclone wa latest d...             Carmel Boyd\n7      tomorrow pm narendra modi ji travel west benga...        PadmalochanPanda\n8      year saw corona watched earthquake saw amphons...            aniltripathi\n9      sucs amphan 120 km east paradip odisha 1030 is...                    ‡§Æ‡•å‡§∏‡§Æ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>full_text_processed</th>\n      <th>name</th>\n    </tr>\n    <tr>\n      <th>Topic</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>despite early warning hurricane amfan mamata a...</td>\n      <td>Chowkidar Archya Midya</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>first cyclone hard situation back front ha arr...</td>\n      <td>Moloy Ghatak</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>thank sharing detail apology delay restricted ...</td>\n      <td>Tata Sky</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>npr budget 3941 cr delhi power corridor 20000 ...</td>\n      <td>Citizen Bapan Das</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>least 12 dead three million evacuee india bang...</td>\n      <td>S√≠ntesis TV</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>786rizwankhan ja bhai apna kam kar kisne bola ...</td>\n      <td>Mohammed kaif</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>climate crisis isnt coming cyclone wa latest d...</td>\n      <td>Carmel Boyd</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>tomorrow pm narendra modi ji travel west benga...</td>\n      <td>PadmalochanPanda</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>year saw corona watched earthquake saw amphons...</td>\n      <td>aniltripathi</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>sucs amphan 120 km east paradip odisha 1030 is...</td>\n      <td>‡§Æ‡•å‡§∏‡§Æ</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 214
    }
   ],
   "source": [
    "g.groupby('Topic').head(1)[['Topic', 'full_text_processed', 'name']].set_index('Topic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top N users for each topic\n",
    "Extracting the top N users for each topic by count of tweets for every user attributed to a topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                     name\nTopic                                                    \n0     The Wealth Home - Let's start building wealth   253\n      MEDIAonINDIA                                    140\n      ABP Ananda                                      121\n      Oneindia Bengali                                 37\n      News18Bangla                                     35\n      Hindustan Times                                  35\n      CPI(M) WEST BENGAL                               32\n      sujoy pal                                        31\n      S Newz                                           26\n      Newspapers                                       23\n1     The Wealth Home - Let's start building wealth   167\n      MEDIAonINDIA                                    121\n      Madhusudan Roy                                   73\n      TIMES NOW                                        57\n      ABP Ananda                                       40\n      Jyotika  Sharma                                  34\n      Safijuddin Ali                                   31\n      arunima                                          29\n      JAGORANI_NGO                                     28\n      Hindustan Times                                  25\n2     The Wealth Home - Let's start building wealth   323\n      Tata Sky                                        274\n      MEDIAonINDIA                                    163\n      BSNL India                                       48\n      ABP Ananda                                       37\n      Oneindia Bengali                                 23\n      The Green Light                                  21\n      Tazaa Khabar                                     19\n      SANDIP Ghosal                                    19\n      InsiderSpirit                                    18\n3     The Wealth Home - Let's start building wealth   105\n      MEDIAonINDIA                                     79\n      Hindustan Times                                  67\n      Sandipan Mitra                                   44\n      ABP Ananda                                       32\n      The Times Of India                               32\n      The Quint                                        31\n      CNNNews18                                        27\n      #India Important India News                      27\n      News18.com                                       26\n4     MEDIAonINDIA                                    214\n      The Wealth Home - Let's start building wealth   184\n      Hindustan Times                                  41\n      infoitesteri                                     41\n      Business Journal                                 32\n      #India Important India News                      27\n      Zyite.news                                       26\n      ALL About Gokak                                  22\n      Abhijit Sarkar                                   20\n      Nachrichtenportal - Freie Welt                   19\n5     The Wealth Home - Let's start building wealth   340\n      MEDIAonINDIA                                    133\n      ABP Ananda                                       47\n      Â§ßÂ§©‰ΩøÁ≥ñÂêõ‚Ä¶„ÄÇ‚ù§Ô∏èüí¢                                       38\n      nerdmela                                         37\n      Mohit Gupta                                      36\n      News18Bangla                                     26\n      Oneindia Bengali                                 25\n      ORF                                              20\n      InsiderSpirit                                    18\n6     The Wealth Home - Let's start building wealth   309\n      MEDIAonINDIA                                    128\n      ABP Ananda                                       70\n      News18Bangla                                     32\n      Oneindia Bengali                                 31\n      JioCare                                          29\n      Â§ßÂ§©‰ΩøÁ≥ñÂêõ‚Ä¶„ÄÇ‚ù§Ô∏èüí¢                                       25\n      Ranjeet Chatterjee                               22\n      S Newz                                           22\n      sujoy pal                                        22\n7     Bharti Airtel India                             707\n      The Wealth Home - Let's start building wealth    90\n      MEDIAonINDIA                                     75\n      Hindustan Times                                  49\n      Hindi Samachar (‡§π‡§ø‡§®‡•ç‡§¶‡•Ä ‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞) News              36\n      zee24ghanta                                      35\n      netvani                                          33\n      News Aryavart                                    31\n      Sher Bahadur                                     26\n      AajTak                                           25\n8     The Wealth Home - Let's start building wealth   460\n      MEDIAonINDIA                                    411\n      Â§ßÂ§©‰ΩøÁ≥ñÂêõ‚Ä¶„ÄÇ‚ù§Ô∏èüí¢                                       28\n      InsiderSpirit                                    27\n      S Newz                                           27\n      SkymetWeather                                    19\n      ABP Ananda                                       17\n      sujoy pal                                        17\n      Sara Bangla                                      16\n      Hindustan Times                                  15\n9     The Wealth Home - Let's start building wealth   146\n      USGS HDDS                                        75\n      Hindustan Times                                  66\n      MEDIAonINDIA                                     64\n      ABP Ananda                                       57\n      SkymetWeather                                    53\n      evewin lakra                                     34\n      News18Bangla                                     26\n      AajTak                                           25\n      sujoy pal                                        24",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>name</th>\n    </tr>\n    <tr>\n      <th>Topic</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"10\" valign=\"top\">0</th>\n      <th>The Wealth Home - Let's start building wealth</th>\n      <td>253</td>\n    </tr>\n    <tr>\n      <th>MEDIAonINDIA</th>\n      <td>140</td>\n    </tr>\n    <tr>\n      <th>ABP Ananda</th>\n      <td>121</td>\n    </tr>\n    <tr>\n      <th>Oneindia Bengali</th>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>News18Bangla</th>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>Hindustan Times</th>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>CPI(M) WEST BENGAL</th>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>sujoy pal</th>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>S Newz</th>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>Newspapers</th>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th rowspan=\"10\" valign=\"top\">1</th>\n      <th>The Wealth Home - Let's start building wealth</th>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>MEDIAonINDIA</th>\n      <td>121</td>\n    </tr>\n    <tr>\n      <th>Madhusudan Roy</th>\n      <td>73</td>\n    </tr>\n    <tr>\n      <th>TIMES NOW</th>\n      <td>57</td>\n    </tr>\n    <tr>\n      <th>ABP Ananda</th>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>Jyotika  Sharma</th>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>Safijuddin Ali</th>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>arunima</th>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>JAGORANI_NGO</th>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>Hindustan Times</th>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th rowspan=\"10\" valign=\"top\">2</th>\n      <th>The Wealth Home - Let's start building wealth</th>\n      <td>323</td>\n    </tr>\n    <tr>\n      <th>Tata Sky</th>\n      <td>274</td>\n    </tr>\n    <tr>\n      <th>MEDIAonINDIA</th>\n      <td>163</td>\n    </tr>\n    <tr>\n      <th>BSNL India</th>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>ABP Ananda</th>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>Oneindia Bengali</th>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>The Green Light</th>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>Tazaa Khabar</th>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>SANDIP Ghosal</th>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>InsiderSpirit</th>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th rowspan=\"10\" valign=\"top\">3</th>\n      <th>The Wealth Home - Let's start building wealth</th>\n      <td>105</td>\n    </tr>\n    <tr>\n      <th>MEDIAonINDIA</th>\n      <td>79</td>\n    </tr>\n    <tr>\n      <th>Hindustan Times</th>\n      <td>67</td>\n    </tr>\n    <tr>\n      <th>Sandipan Mitra</th>\n      <td>44</td>\n    </tr>\n    <tr>\n      <th>ABP Ananda</th>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>The Times Of India</th>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>The Quint</th>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>CNNNews18</th>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>#India Important India News</th>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>News18.com</th>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th rowspan=\"10\" valign=\"top\">4</th>\n      <th>MEDIAonINDIA</th>\n      <td>214</td>\n    </tr>\n    <tr>\n      <th>The Wealth Home - Let's start building wealth</th>\n      <td>184</td>\n    </tr>\n    <tr>\n      <th>Hindustan Times</th>\n      <td>41</td>\n    </tr>\n    <tr>\n      <th>infoitesteri</th>\n      <td>41</td>\n    </tr>\n    <tr>\n      <th>Business Journal</th>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>#India Important India News</th>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>Zyite.news</th>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>ALL About Gokak</th>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>Abhijit Sarkar</th>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>Nachrichtenportal - Freie Welt</th>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th rowspan=\"10\" valign=\"top\">5</th>\n      <th>The Wealth Home - Let's start building wealth</th>\n      <td>340</td>\n    </tr>\n    <tr>\n      <th>MEDIAonINDIA</th>\n      <td>133</td>\n    </tr>\n    <tr>\n      <th>ABP Ananda</th>\n      <td>47</td>\n    </tr>\n    <tr>\n      <th>Â§ßÂ§©‰ΩøÁ≥ñÂêõ‚Ä¶„ÄÇ‚ù§Ô∏èüí¢</th>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>nerdmela</th>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>Mohit Gupta</th>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>News18Bangla</th>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>Oneindia Bengali</th>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>ORF</th>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>InsiderSpirit</th>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th rowspan=\"10\" valign=\"top\">6</th>\n      <th>The Wealth Home - Let's start building wealth</th>\n      <td>309</td>\n    </tr>\n    <tr>\n      <th>MEDIAonINDIA</th>\n      <td>128</td>\n    </tr>\n    <tr>\n      <th>ABP Ananda</th>\n      <td>70</td>\n    </tr>\n    <tr>\n      <th>News18Bangla</th>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>Oneindia Bengali</th>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>JioCare</th>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>Â§ßÂ§©‰ΩøÁ≥ñÂêõ‚Ä¶„ÄÇ‚ù§Ô∏èüí¢</th>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>Ranjeet Chatterjee</th>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>S Newz</th>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>sujoy pal</th>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th rowspan=\"10\" valign=\"top\">7</th>\n      <th>Bharti Airtel India</th>\n      <td>707</td>\n    </tr>\n    <tr>\n      <th>The Wealth Home - Let's start building wealth</th>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>MEDIAonINDIA</th>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>Hindustan Times</th>\n      <td>49</td>\n    </tr>\n    <tr>\n      <th>Hindi Samachar (‡§π‡§ø‡§®‡•ç‡§¶‡•Ä ‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞) News</th>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>zee24ghanta</th>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>netvani</th>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>News Aryavart</th>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>Sher Bahadur</th>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>AajTak</th>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th rowspan=\"10\" valign=\"top\">8</th>\n      <th>The Wealth Home - Let's start building wealth</th>\n      <td>460</td>\n    </tr>\n    <tr>\n      <th>MEDIAonINDIA</th>\n      <td>411</td>\n    </tr>\n    <tr>\n      <th>Â§ßÂ§©‰ΩøÁ≥ñÂêõ‚Ä¶„ÄÇ‚ù§Ô∏èüí¢</th>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>InsiderSpirit</th>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>S Newz</th>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>SkymetWeather</th>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>ABP Ananda</th>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>sujoy pal</th>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>Sara Bangla</th>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>Hindustan Times</th>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th rowspan=\"10\" valign=\"top\">9</th>\n      <th>The Wealth Home - Let's start building wealth</th>\n      <td>146</td>\n    </tr>\n    <tr>\n      <th>USGS HDDS</th>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>Hindustan Times</th>\n      <td>66</td>\n    </tr>\n    <tr>\n      <th>MEDIAonINDIA</th>\n      <td>64</td>\n    </tr>\n    <tr>\n      <th>ABP Ananda</th>\n      <td>57</td>\n    </tr>\n    <tr>\n      <th>SkymetWeather</th>\n      <td>53</td>\n    </tr>\n    <tr>\n      <th>evewin lakra</th>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>News18Bangla</th>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>AajTak</th>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>sujoy pal</th>\n      <td>24</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 239
    }
   ],
   "source": [
    "topic_users = tweet_topics_df.groupby(['Topic'])['name'].apply(lambda x: x.value_counts().head(10)).to_frame()\n",
    "topic_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_prepared_1 = pyLDAvis.gensim.prepare(lda_model_1, tfidf_corpus, tweets_dict)\n",
    "LDAvis_prepared_2 = pyLDAvis.gensim.prepare(lda_model_2, tfidf_corpus, tweets_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the HTML\n",
    "pyLDAvis.save_html(LDAvis_prepared_1, '../reports/figures/LDA_topic_10.html')\n",
    "pyLDAvis.save_html(LDAvis_prepared_2, '../reports/figures/LDA_topic_6.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}